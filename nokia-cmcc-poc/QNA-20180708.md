# Question and Answer 2018/07/08 
Date: 20180708

|Case|Description|Question|Answer|
|:--:|:----------|:-------|:-----|
|6.6.1.1|虚拟交换机绑核|如何查看和配置虚拟交换机绑核情况？|ovs-vsctl get Open_vSwitch . other_config，检查输出里pmd-cpu-mask相关部分
|6.4.3.1|NUMA亲和性|测试预期结果2中，要求NUMA节点内存的free数值变化和虚拟机内存占用情况相符, 如何判断相符是合理的？我们的测试情况：创建实例前物理主机：node 0 free：9700mb，创建内存为4G的虚机后物理主机：node 0 free：8963mb, 只减少了700多兆|目前实例虚拟内存占用的物理内存默认不是分配即独享占用的。如果希望分配时独享占用物理内存一个可能的方式是使用hugepage，物理节点启用hugepage，flavor启用hugepage
|6.4.9.1|虚拟机开启与关闭|我们在nova.conf文件中增加shutdown_timeout=0的配置后，重启nova-compute服务，进入虚拟机输入poweroff进行关机会导致卡死无法关机，去掉配置shutdown_timeout=60后，虚拟机可以正常关机，然后再配上shutdown_timeout=60页面上点击开机，虚拟机是没有反应的。这个是否是bug? 如何解决?|建议开case咨询
|5.4.6.2|SR-IOV MTU可调整|按照步骤如何达到预期结果5、6、7就行。|可以参考以下命令包装脚本<br>调用顺序：<br>1. 在所有控制节点上执行控制节点命令，参数为MTU<br>2. 在相关计算节点上执行计算节点命令，参数为MTU，pf_nic_name<br><br>参见控制节点命令例子<br>参见计算节点命令例子
|6.5.1|存储多AZ|是否支持在一个cinder.conf里可否配多个存储AZ，每个存贮AZ对应不同的storage_pool？Case要求是选择Availability Zones AZ1、AZ2分别创建虚拟机AZ1-VM、AZ2-VM要求在不同AZ上建的实例要存放在对应的存贮az和对应的pool里，应如何配置？|不支持在cinder.conf里storage_availability_zone设置多个AZ，可能的实现逻辑是配置多个cinder-volume服务，每个服务对应不同的AZ，每个AZ有自己的storage_pool。
|10.1.3.2|虚拟化资源预留|用例要求实现虚拟化资源预留能力，请问是否可以做到？如何进行？|界面部分不支持，手工设置方法为对于AZ1下HA1所有计算节点设置nova.conf中的cpu_allocation_ratio为测试用例中指定值并重启openstack-nova-compute服务
|9.1.2|虚拟机引导方式|能否提供用例要求的，支持UEFI启动模式镜像？|UEFI启动依赖OVMF，目前OSP10尚不支持OVMF。建议开case。参见：https://blueprints.launchpad.net/nova/+spec/boot-from-uefi
|6.2.4.2|按项目进行配额管理|某些项目的配额默认值不满足用例要求，且无法修改。例如backup_gigabytes, 用例要求-1，但目前系统中默认是1000，且无法修改。请问如何修改这些配额默认值？|建议开case<br>`$ openstack quota set --backup-gigabytes -1 PROJECT_ID` 
|6.2.5.2|支持基于角色的访问控制|用例要求支持projectadmin权限的用户，projectadmin拥有对本租户、本租户内的用户/用户组、本租户内Role分配情况、本租户的配额的查看权限。不支持其他租户的用户/用户组的查看。请问如何实现用例要求？|需定制projecadmin角色，具体设置建议开case


###配置MTU命令
####控制节点命令例子
```
echo 'dhcp-option-force=26,9000' > /etc/neutron/dnsmasq-neutron.conf
/usr/bin/crudini --set /etc/neutron/neutron.conf DEFAULT network_device_mtu 9000
/usr/bin/crudini --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 path_mtu 9000
systemctl restart neutron-*
```

####计算节点命令例子
```
/usr/bin/crudini --set /etc/neutron/neutron.conf DEFAULT network_device_mtu '9000'
systemctl restart neutron-openvswitch-agent
ip link set dev <pf_nic_name> mtu 9000
cat /etc/sysconfig/network-scripts/ifcfg-<pf_nic_name> | grep -v MTU | tee /etc/sysconfig/network-scripts/ifcfg-<pf_nic_name>.mtu
echo 'MTU=9000' >> /etc/sysconfig/network-scripts/ifcfg-<pf_nic_name>.mtu`<br>`cp -n /etc/sysconfig/network-scripts/ifcfg-<pf_nic_name>.mtu /etc/sysconfig/network-scripts/ifcfg-<pf_nic_name>
```
